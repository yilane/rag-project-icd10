#!/usr/bin/env python3
"""
å±‚çº§ç›¸ä¼¼åº¦è®¡ç®—æœåŠ¡
åŸºäºICD-10å±‚çº§ç»“æ„çš„å¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—å’Œè¯„åˆ†å¢å¼º
"""

import os
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from loguru import logger
from sklearn.metrics.pairwise import cosine_similarity


@dataclass
class SimilarityFactors:
    """ç›¸ä¼¼åº¦è®¡ç®—å› å­"""
    vector_similarity: float = 0.0      # å‘é‡ç›¸ä¼¼åº¦
    hierarchy_boost: float = 0.0        # å±‚çº§å¢å¼ºåˆ†æ•°
    entity_match_score: float = 0.0     # å®ä½“åŒ¹é…åˆ†æ•°  
    semantic_coherence: float = 0.0     # è¯­ä¹‰ä¸€è‡´æ€§
    category_alignment: float = 0.0     # ç±»åˆ«å¯¹é½åº¦
    context_relevance: float = 0.0      # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
    
    def __post_init__(self):
        """ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯PythonåŸç”Ÿfloatç±»å‹"""
        self.vector_similarity = float(self.vector_similarity)
        self.hierarchy_boost = float(self.hierarchy_boost)
        self.entity_match_score = float(self.entity_match_score)
        self.semantic_coherence = float(self.semantic_coherence)
        self.category_alignment = float(self.category_alignment)
        self.context_relevance = float(self.context_relevance)


@dataclass
class HierarchyInfo:
    """ICD-10å±‚çº§ä¿¡æ¯"""
    level: int = 1
    parent_code: str = ""
    category_path: str = ""
    main_category: str = ""
    sub_category: str = ""
    semantic_keywords: List[str] = None
    
    def __post_init__(self):
        if self.semantic_keywords is None:
            self.semantic_keywords = []


class HierarchicalSimilarityService:
    """å±‚çº§ç›¸ä¼¼åº¦è®¡ç®—æœåŠ¡"""
    
    def __init__(self, embedding_service=None, ner_service=None):
        """
        åˆå§‹åŒ–å±‚çº§ç›¸ä¼¼åº¦æœåŠ¡
        
        Args:
            embedding_service: åµŒå…¥æœåŠ¡å®ä¾‹
            ner_service: å‘½åå®ä½“è¯†åˆ«æœåŠ¡å®ä¾‹ 
        """
        self.embedding_service = embedding_service
        self.ner_service = ner_service
        
        # å±‚çº§æƒé‡é…ç½®ï¼ˆåŸºäºç°æœ‰ç³»ç»Ÿï¼‰
        self.level_weights = {
            1: 1.2,  # ä¸»ç±»åˆ« - æƒé‡æ›´é«˜
            2: 1.0,  # äºšç±»åˆ« - æ ‡å‡†æƒé‡  
            3: 0.8   # ç»†åˆ†ç±» - æƒé‡è¾ƒä½
        }
        
        # ç›¸ä¼¼åº¦å› å­æƒé‡é…ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        self.factor_weights = {
            'vector_similarity': 0.50,      # åŸºç¡€å‘é‡ç›¸ä¼¼åº¦ - æé«˜æƒé‡
            'hierarchy_boost': 0.20,        # å±‚çº§å¢å¼ºåˆ†æ•° - é€‚ä¸­æƒé‡
            'entity_match_score': 0.15,     # å®ä½“åŒ¹é…åˆ†æ•° - é€‚ä¸­æƒé‡
            'semantic_coherence': 0.08,     # è¯­ä¹‰ä¸€è‡´æ€§ - é™ä½æƒé‡
            'category_alignment': 0.04,     # ç±»åˆ«å¯¹é½åº¦ - é™ä½æƒé‡
            'context_relevance': 0.03       # ä¸Šä¸‹æ–‡ç›¸å…³æ€§ - é™ä½æƒé‡
        }
        
        # ICD-10ä¸»ç±»åˆ«æ˜ å°„ï¼ˆç”¨äºè¯­ä¹‰å¢å¼ºï¼‰
        self.main_categories = self._load_main_categories()
        
        # ç›¸ä¼¼åº¦è®¡ç®—ç¼“å­˜
        self.similarity_cache = {}
        
        logger.info(f"å±‚çº§ç›¸ä¼¼åº¦æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œæƒé‡é…ç½®: {self.factor_weights}")
    
    def _load_main_categories(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½ICD-10ä¸»ç±»åˆ«ä¿¡æ¯"""
        return {
            'A': {
                'name': 'æŸäº›ä¼ æŸ“ç—…å’Œå¯„ç”Ÿè™«ç—…',
                'keywords': ['æ„ŸæŸ“', 'ä¼ æŸ“', 'ç—…æ¯’', 'ç»†èŒ', 'å¯„ç”Ÿè™«', 'çœŸèŒ'],
                'semantic_weight': 1.1
            },
            'B': {
                'name': 'è‚¿ç˜¤', 
                'keywords': ['ç™Œ', 'ç˜¤', 'è‚¿ç˜¤', 'æ¶æ€§', 'è‰¯æ€§', 'è½¬ç§»'],
                'semantic_weight': 1.2
            },
            'C': {
                'name': 'è¡€æ¶²åŠé€ è¡€å™¨å®˜ç–¾ç—…',
                'keywords': ['è¡€æ¶²', 'è´«è¡€', 'ç™½è¡€ç—…', 'å‡ºè¡€', 'å‡è¡€'],
                'semantic_weight': 1.0
            },
            'E': {
                'name': 'å†…åˆ†æ³Œã€è¥å…»å’Œä»£è°¢ç–¾ç—…',
                'keywords': ['ç³–å°¿ç—…', 'ç”²çŠ¶è…º', 'ä»£è°¢', 'å†…åˆ†æ³Œ', 'è¥å…»'],
                'semantic_weight': 1.1
            },
            'I': {
                'name': 'å¾ªç¯ç³»ç»Ÿç–¾ç—…',
                'keywords': ['å¿ƒè„', 'è¡€ç®¡', 'é«˜è¡€å‹', 'å¿ƒè‚Œ', 'å¾ªç¯'],
                'semantic_weight': 1.2
            },
            'J': {
                'name': 'å‘¼å¸ç³»ç»Ÿç–¾ç—…',
                'keywords': ['è‚º', 'å‘¼å¸', 'å’³å—½', 'æ°”ç®¡', 'æ”¯æ°”ç®¡'],
                'semantic_weight': 1.1
            },
            'K': {
                'name': 'æ¶ˆåŒ–ç³»ç»Ÿç–¾ç—…',
                'keywords': ['èƒƒ', 'è‚ ', 'è‚', 'æ¶ˆåŒ–', 'è…¹æ³»'],
                'semantic_weight': 1.0
            },
            'N': {
                'name': 'æ³Œå°¿ç”Ÿæ®–ç³»ç»Ÿç–¾ç—…',
                'keywords': ['è‚¾', 'è†€èƒ±', 'æ³Œå°¿', 'ç”Ÿæ®–', 'å°¿'],
                'semantic_weight': 1.0
            },
            'S': {
                'name': 'æŸä¼¤ã€ä¸­æ¯’å’Œå¤–å› çš„æŸäº›å…¶ä»–åæœ',
                'keywords': ['æŸä¼¤', 'å¤–ä¼¤', 'éª¨æŠ˜', 'ä¸­æ¯’', 'çƒ§ä¼¤'],
                'semantic_weight': 0.9
            }
        }
    
    def calculate_enhanced_similarity(self, 
                                    query_text: str,
                                    query_entities: Dict[str, List[Dict]], 
                                    candidate_record: Dict[str, Any]) -> Tuple[float, SimilarityFactors]:
        """
        è®¡ç®—å¢å¼ºçš„å±‚çº§ç›¸ä¼¼åº¦
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_entities: æŸ¥è¯¢æ–‡æœ¬çš„å®ä½“ä¿¡æ¯
            candidate_record: å€™é€‰ICDè®°å½•
            
        Returns:
            å¢å¼ºç›¸ä¼¼åº¦åˆ†æ•°å’Œè¯¦ç»†å› å­
        """
        factors = SimilarityFactors()
        
        try:
            # 1. åŸºç¡€å‘é‡ç›¸ä¼¼åº¦
            factors.vector_similarity = self._calculate_vector_similarity(
                query_text, candidate_record
            )
            
            # 2. å±‚çº§å¢å¼ºåˆ†æ•°
            factors.hierarchy_boost = self._calculate_hierarchy_boost(
                query_text, query_entities, candidate_record
            )
            
            # 3. å®ä½“åŒ¹é…åˆ†æ•°
            factors.entity_match_score = self._calculate_entity_match_score(
                query_entities, candidate_record
            )
            
            # 4. è¯­ä¹‰ä¸€è‡´æ€§
            factors.semantic_coherence = self._calculate_semantic_coherence(
                query_text, candidate_record
            )
            
            # 5. ç±»åˆ«å¯¹é½åº¦
            factors.category_alignment = self._calculate_category_alignment(
                query_entities, candidate_record
            )
            
            # 6. ä¸Šä¸‹æ–‡ç›¸å…³æ€§
            factors.context_relevance = self._calculate_context_relevance(
                query_text, candidate_record
            )
            
            # è®¡ç®—åŠ æƒæ€»åˆ†
            enhanced_score = self._calculate_weighted_score(factors)
            
            logger.debug(f"å¢å¼ºç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ: {candidate_record.get('code', 'unknown')} = {enhanced_score:.4f}")
            
            return float(enhanced_score), factors
            
        except Exception as e:
            logger.error(f"å¢å¼ºç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€ç›¸ä¼¼åº¦ä½œä¸ºå›é€€
            base_score = candidate_record.get('score', 0.0)
            return float(base_score), factors
    
    def _calculate_vector_similarity(self, query_text: str, candidate_record: Dict[str, Any]) -> float:
        """è®¡ç®—åŸºç¡€å‘é‡ç›¸ä¼¼åº¦"""
        try:
            if not self.embedding_service:
                return candidate_record.get('score', 0.0)
            
            # ä½¿ç”¨å·²æœ‰çš„å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œæˆ–é‡æ–°è®¡ç®—
            if 'score' in candidate_record:
                return float(candidate_record['score'])
            
            # å¦‚æœéœ€è¦é‡æ–°è®¡ç®—
            query_vector = self.embedding_service.encode_query(query_text)
            candidate_text = candidate_record.get('semantic_text', candidate_record.get('preferred_zh', ''))
            candidate_vector = self.embedding_service.encode_query(candidate_text)
            
            similarity = cosine_similarity([query_vector], [candidate_vector])[0][0]
            return float(max(similarity, 0.0))
            
        except Exception as e:
            logger.warning(f"å‘é‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return candidate_record.get('score', 0.0)
    
    def _calculate_hierarchy_boost(self, 
                                 query_text: str,
                                 query_entities: Dict[str, List[Dict]],
                                 candidate_record: Dict[str, Any]) -> float:
        """è®¡ç®—å±‚çº§å¢å¼ºåˆ†æ•°"""
        boost_score = 0.0
        
        try:
            # è·å–å€™é€‰è®°å½•çš„å±‚çº§ä¿¡æ¯
            level = candidate_record.get('level', 1)
            code = candidate_record.get('code', '')
            parent_code = candidate_record.get('parent_code', '')
            category_path = candidate_record.get('category_path', '')
            
            # åŸºäºå±‚çº§çš„åŸºç¡€å¢å¼º
            level_boost = self._get_level_boost_factor(level)
            boost_score += level_boost * 0.3
            
            # ä¸»ç±»åˆ«è¯­ä¹‰åŒ¹é…å¢å¼º
            main_category_code = code[0] if code else ''
            if main_category_code in self.main_categories:
                category_info = self.main_categories[main_category_code]
                category_boost = self._calculate_category_semantic_boost(
                    query_text, query_entities, category_info
                )
                boost_score += category_boost * 0.4
            
            # çˆ¶å­å…³ç³»å¢å¼º
            if parent_code:
                parent_boost = self._calculate_parent_child_boost(
                    query_entities, code, parent_code
                )
                boost_score += parent_boost * 0.3
            
            return float(min(boost_score, 0.3))  # é™åˆ¶æœ€å¤§å¢å¼ºåˆ†æ•°ï¼Œé¿å…è¿‡åº¦å¢å¼º
            
        except Exception as e:
            logger.warning(f"å±‚çº§å¢å¼ºåˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _get_level_boost_factor(self, level: int) -> float:
        """è·å–å±‚çº§å¢å¼ºå› å­"""
        # ä¸åŒå±‚çº§çš„å¢å¼ºç­–ç•¥
        level_boost_factors = {
            1: 0.15,  # ä¸»ç±»åˆ« - é€‚ä¸­å¢å¼º
            2: 0.20,  # äºšç±»åˆ« - æœ€å¤§å¢å¼ºï¼ˆå¹³è¡¡ç‚¹ï¼‰
            3: 0.10   # ç»†åˆ†ç±» - è¾ƒå°å¢å¼º  
        }
        return float(level_boost_factors.get(level, 0.10))
    
    def _calculate_category_semantic_boost(self,
                                         query_text: str,
                                         query_entities: Dict[str, List[Dict]], 
                                         category_info: Dict[str, Any]) -> float:
        """è®¡ç®—ç±»åˆ«è¯­ä¹‰å¢å¼ºåˆ†æ•°"""
        boost = 0.0
        
        try:
            category_keywords = category_info.get('keywords', [])
            semantic_weight = category_info.get('semantic_weight', 1.0)
            
            # æ£€æŸ¥æŸ¥è¯¢æ–‡æœ¬ä¸­çš„ç±»åˆ«å…³é”®è¯åŒ¹é…
            query_lower = query_text.lower()
            matched_keywords = 0
            for keyword in category_keywords:
                if keyword in query_lower:
                    matched_keywords += 1
            
            if matched_keywords > 0:
                keyword_boost = (matched_keywords / len(category_keywords)) * 0.3
                boost += keyword_boost * semantic_weight
            
            # æ£€æŸ¥ç–¾ç—…å®ä½“ä¸ç±»åˆ«çš„åŒ¹é…åº¦
            disease_entities = query_entities.get('disease', [])
            for entity in disease_entities:
                entity_text = entity.get('text', '').lower()
                entity_matches = sum(1 for kw in category_keywords if kw in entity_text)
                if entity_matches > 0:
                    entity_boost = (entity_matches / len(category_keywords)) * 0.2
                    boost += entity_boost * entity.get('confidence', 0.5)
            
            return float(min(boost, 0.4))
            
        except Exception as e:
            logger.warning(f"ç±»åˆ«è¯­ä¹‰å¢å¼ºè®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_parent_child_boost(self,
                                    query_entities: Dict[str, List[Dict]],
                                    code: str,
                                    parent_code: str) -> float:
        """è®¡ç®—çˆ¶å­å…³ç³»å¢å¼ºåˆ†æ•°"""
        # ç®€åŒ–çš„çˆ¶å­å…³ç³»å¢å¼º
        # å®é™…å®ç°ä¸­å¯ä»¥åŸºäºICD-10çš„å…·ä½“å±‚çº§å…³ç³»è¿›è¡Œæ›´å¤æ‚çš„è®¡ç®—
        if len(code) > len(parent_code) and code.startswith(parent_code):
            return 0.1  # ç¡®å®æ˜¯çˆ¶å­å…³ç³»
        return 0.0
    
    def _calculate_entity_match_score(self,
                                    query_entities: Dict[str, List[Dict]],
                                    candidate_record: Dict[str, Any]) -> float:
        """è®¡ç®—å®ä½“åŒ¹é…åˆ†æ•°"""
        match_score = 0.0
        
        try:
            candidate_text = candidate_record.get('preferred_zh', '').lower()
            semantic_text = candidate_record.get('semantic_text', '').lower()
            combined_text = f"{candidate_text} {semantic_text}"
            
            # ç–¾ç—…å®ä½“åŒ¹é…ï¼ˆæƒé‡æœ€é«˜ï¼‰
            disease_entities = query_entities.get('disease', [])
            for entity in disease_entities:
                entity_text = entity.get('text', '').lower()
                confidence = entity.get('confidence', 0.5)
                
                if entity_text in combined_text:
                    match_score += confidence * 0.4
                elif any(word in combined_text for word in entity_text.split()):
                    match_score += confidence * 0.2
            
            # ç—‡çŠ¶å®ä½“åŒ¹é…
            symptom_entities = query_entities.get('symptom', [])
            for entity in symptom_entities:
                entity_text = entity.get('text', '').lower()
                confidence = entity.get('confidence', 0.5)
                
                if entity_text in combined_text:
                    match_score += confidence * 0.2
            
            # è§£å‰–éƒ¨ä½åŒ¹é…
            anatomy_entities = query_entities.get('anatomy', [])
            for entity in anatomy_entities:
                entity_text = entity.get('text', '').lower()
                confidence = entity.get('confidence', 0.5)
                
                if entity_text in combined_text:
                    match_score += confidence * 0.1
            
            return float(min(match_score, 1.0))
            
        except Exception as e:
            logger.warning(f"å®ä½“åŒ¹é…åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_semantic_coherence(self,
                                    query_text: str,
                                    candidate_record: Dict[str, Any]) -> float:
        """è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§"""
        try:
            if not self.embedding_service:
                return 0.5
            
            # ä½¿ç”¨è¯­ä¹‰æ–‡æœ¬è¿›è¡Œæ›´ç²¾ç¡®çš„ä¸€è‡´æ€§è®¡ç®—
            candidate_semantic = candidate_record.get('semantic_text', '')
            if not candidate_semantic:
                return 0.3
            
            # è®¡ç®—è¯­ä¹‰å‘é‡çš„ä¸€è‡´æ€§
            query_vector = self.embedding_service.encode_query(query_text)
            semantic_vector = self.embedding_service.encode_query(candidate_semantic)
            
            coherence = cosine_similarity([query_vector], [semantic_vector])[0][0]
            return max(coherence, 0.0)
            
        except Exception as e:
            logger.warning(f"è¯­ä¹‰ä¸€è‡´æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_category_alignment(self,
                                    query_entities: Dict[str, List[Dict]],
                                    candidate_record: Dict[str, Any]) -> float:
        """è®¡ç®—ç±»åˆ«å¯¹é½åº¦"""
        try:
            code = candidate_record.get('code', '')
            if not code:
                return 0.0
            
            main_category = code[0]
            if main_category not in self.main_categories:
                return 0.0
            
            category_info = self.main_categories[main_category]
            category_keywords = category_info.get('keywords', [])
            
            # æ£€æŸ¥æŸ¥è¯¢å®ä½“ä¸ç±»åˆ«çš„å¯¹é½ç¨‹åº¦
            alignment_score = 0.0
            total_entities = 0
            
            for entity_type, entities in query_entities.items():
                for entity in entities:
                    total_entities += 1
                    entity_text = entity.get('text', '').lower()
                    
                    # æ£€æŸ¥å®ä½“æ˜¯å¦ä¸ç±»åˆ«å…³é”®è¯å¯¹é½
                    for keyword in category_keywords:
                        if keyword in entity_text:
                            alignment_score += entity.get('confidence', 0.5)
                            break
            
            return float(alignment_score / total_entities) if total_entities > 0 else 0.0
            
        except Exception as e:
            logger.warning(f"ç±»åˆ«å¯¹é½åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_context_relevance(self,
                                   query_text: str,
                                   candidate_record: Dict[str, Any]) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡ç›¸å…³æ€§"""
        try:
            # ç®€åŒ–çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§è®¡ç®—
            # åŸºäºæ–‡æœ¬é•¿åº¦ã€å¤æ‚åº¦ç­‰å› ç´ 
            
            query_length = len(query_text)
            candidate_text = candidate_record.get('preferred_zh', '')
            candidate_length = len(candidate_text)
            
            # é•¿åº¦ç›¸ä¼¼æ€§
            length_similarity = 1.0 - abs(query_length - candidate_length) / max(query_length, candidate_length, 1)
            
            # å¤æ‚åº¦åŒ¹é…ï¼ˆåŸºäºå­—ç¬¦å¤šæ ·æ€§ï¼‰
            query_chars = set(query_text)
            candidate_chars = set(candidate_text)
            char_overlap = len(query_chars & candidate_chars) / len(query_chars | candidate_chars) if (query_chars | candidate_chars) else 0
            
            relevance = (length_similarity * 0.3 + char_overlap * 0.7)
            return max(relevance, 0.0)
            
        except Exception as e:
            logger.warning(f"ä¸Šä¸‹æ–‡ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_weighted_score(self, factors: SimilarityFactors) -> float:
        """è®¡ç®—åŠ æƒæ€»åˆ†ï¼ˆé‡‡ç”¨åŠ æ³•å¢å¼ºæ¨¡å¼ï¼‰"""
        try:
            # åŸºç¡€å‘é‡ç›¸ä¼¼åº¦ä½œä¸ºèµ·ç‚¹
            base_score = factors.vector_similarity
            
            # å„ç§å¢å¼ºå› å­çš„åŠ æ³•è´¡çŒ®
            enhancements = 0.0
            
            # å±‚çº§å¢å¼ºï¼ˆç›´æ¥åŠ åˆ°åŸºç¡€åˆ†æ•°ä¸Šï¼‰
            enhancements += factors.hierarchy_boost * self.factor_weights['hierarchy_boost'] / 0.2  # å½’ä¸€åŒ–åˆ°æƒé‡
            
            # å®ä½“åŒ¹é…å¢å¼º
            enhancements += factors.entity_match_score * self.factor_weights['entity_match_score'] / 0.15
            
            # è¯­ä¹‰ä¸€è‡´æ€§å¢å¼ºï¼ˆå¦‚æœé«˜äºåŸºç¡€åˆ†æ•°ï¼‰
            if factors.semantic_coherence > base_score:
                semantic_boost = (factors.semantic_coherence - base_score) * self.factor_weights['semantic_coherence'] / 0.08
                enhancements += semantic_boost
            
            # ç±»åˆ«å¯¹é½å¢å¼º
            enhancements += factors.category_alignment * self.factor_weights['category_alignment'] / 0.04
            
            # ä¸Šä¸‹æ–‡ç›¸å…³æ€§å¢å¼º
            enhancements += factors.context_relevance * self.factor_weights['context_relevance'] / 0.03
            
            # æœ€ç»ˆåˆ†æ•° = åŸºç¡€åˆ†æ•° + å¢å¼ºåˆ†æ•°
            final_score = base_score + enhancements
            
            return float(min(final_score, 1.8))  # å…è®¸æ˜¾è‘—å¢å¼ºï¼Œä½†è®¾ç½®åˆç†ä¸Šé™
            
        except Exception as e:
            logger.error(f"åŠ æƒåˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return float(factors.vector_similarity)  # å›é€€åˆ°åŸºç¡€å‘é‡ç›¸ä¼¼åº¦
    
    def batch_calculate_similarities(self,
                                   query_text: str,
                                   query_entities: Dict[str, List[Dict]],
                                   candidate_records: List[Dict[str, Any]]) -> List[Tuple[Dict[str, Any], float, SimilarityFactors]]:
        """
        æ‰¹é‡è®¡ç®—å¢å¼ºç›¸ä¼¼åº¦
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_entities: æŸ¥è¯¢å®ä½“
            candidate_records: å€™é€‰è®°å½•åˆ—è¡¨
            
        Returns:
            å¢å¼ºåçš„å€™é€‰è®°å½•åˆ—è¡¨ï¼ŒåŒ…å«åˆ†æ•°å’Œå› å­ä¿¡æ¯
        """
        enhanced_results = []
        
        logger.info(f"å¼€å§‹æ‰¹é‡è®¡ç®— {len(candidate_records)} ä¸ªå€™é€‰è®°å½•çš„å¢å¼ºç›¸ä¼¼åº¦")
        
        for record in candidate_records:
            try:
                enhanced_score, factors = self.calculate_enhanced_similarity(
                    query_text, query_entities, record
                )
                
                # æ›´æ–°è®°å½•çš„åˆ†æ•°
                enhanced_record = record.copy()
                enhanced_record['enhanced_score'] = enhanced_score
                enhanced_record['original_score'] = record.get('score', 0.0)
                enhanced_record['similarity_factors'] = factors
                
                enhanced_results.append((enhanced_record, enhanced_score, factors))
                
            except Exception as e:
                logger.error(f"è®°å½• {record.get('code', 'unknown')} çš„ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                # ä½¿ç”¨åŸå§‹åˆ†æ•°ä½œä¸ºå›é€€
                original_score = record.get('score', 0.0)
                enhanced_results.append((record, original_score, SimilarityFactors()))
        
        # æŒ‰å¢å¼ºåˆ†æ•°æ’åº
        enhanced_results.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆï¼Œå¹³å‡å¢å¼ºåˆ†æ•°: {float(np.mean([r[1] for r in enhanced_results])):.4f}")
        
        return enhanced_results
    
    def get_similarity_explanation(self, factors: SimilarityFactors) -> Dict[str, Any]:
        """è·å–ç›¸ä¼¼åº¦è®¡ç®—çš„è¯¦ç»†è§£é‡Š"""
        explanation = {
            'total_score': self._calculate_weighted_score(factors),
            'factors': {
                'vector_similarity': {
                    'score': factors.vector_similarity,
                    'weight': self.factor_weights['vector_similarity'],
                    'contribution': factors.vector_similarity * self.factor_weights['vector_similarity'],
                    'description': 'åŸºç¡€å‘é‡ç›¸ä¼¼åº¦'
                },
                'hierarchy_boost': {
                    'score': factors.hierarchy_boost,
                    'weight': self.factor_weights['hierarchy_boost'],
                    'contribution': factors.hierarchy_boost * self.factor_weights['hierarchy_boost'],
                    'description': 'ICD-10å±‚çº§å¢å¼ºåˆ†æ•°'
                },
                'entity_match_score': {
                    'score': factors.entity_match_score,
                    'weight': self.factor_weights['entity_match_score'],
                    'contribution': factors.entity_match_score * self.factor_weights['entity_match_score'],
                    'description': 'åŒ»å­¦å®ä½“åŒ¹é…åˆ†æ•°'
                },
                'semantic_coherence': {
                    'score': factors.semantic_coherence,
                    'weight': self.factor_weights['semantic_coherence'],
                    'contribution': factors.semantic_coherence * self.factor_weights['semantic_coherence'],
                    'description': 'è¯­ä¹‰ä¸€è‡´æ€§åˆ†æ•°'
                },
                'category_alignment': {
                    'score': factors.category_alignment,
                    'weight': self.factor_weights['category_alignment'],
                    'contribution': factors.category_alignment * self.factor_weights['category_alignment'],
                    'description': 'ICDç±»åˆ«å¯¹é½åˆ†æ•°'
                },
                'context_relevance': {
                    'score': factors.context_relevance,
                    'weight': self.factor_weights['context_relevance'],
                    'contribution': factors.context_relevance * self.factor_weights['context_relevance'],
                    'description': 'ä¸Šä¸‹æ–‡ç›¸å…³æ€§åˆ†æ•°'
                }
            }
        }
        
        return explanation
    
    def update_weights(self, new_weights: Dict[str, float]):
        """æ›´æ–°æƒé‡é…ç½®"""
        for factor, weight in new_weights.items():
            if factor in self.factor_weights:
                self.factor_weights[factor] = weight
                logger.info(f"æƒé‡æ›´æ–°: {factor} = {weight}")
        
        # ç¡®ä¿æƒé‡å’Œä¸º1
        total_weight = sum(self.factor_weights.values())
        if total_weight != 1.0:
            logger.warning(f"æƒé‡æ€»å’Œä¸ä¸º1.0: {total_weight}ï¼Œè‡ªåŠ¨å½’ä¸€åŒ–")
            for factor in self.factor_weights:
                self.factor_weights[factor] /= total_weight


def main():
    """æµ‹è¯•å‡½æ•°"""
    print("=== å±‚çº§ç›¸ä¼¼åº¦è®¡ç®—æœåŠ¡æµ‹è¯• ===")
    
    # æ¨¡æ‹ŸæœåŠ¡
    class MockEmbeddingService:
        def encode_query(self, text):
            import numpy as np
            # ç®€å•çš„åŸºäºé•¿åº¦å’Œå­—ç¬¦çš„å‘é‡æ¨¡æ‹Ÿ
            vector = np.random.rand(10)
            vector[0] = len(text) / 50.0  # é•¿åº¦ç‰¹å¾
            vector[1] = len(set(text)) / 20.0  # å­—ç¬¦å¤šæ ·æ€§
            return vector
    
    mock_embedding = MockEmbeddingService()
    similarity_service = HierarchicalSimilarityService(mock_embedding)
    
    # æµ‹è¯•æ•°æ®
    query_text = "æ€¥æ€§å¿ƒè‚Œæ¢—æ­»ä¼´å¿ƒå¾‹å¤±å¸¸"
    query_entities = {
        'disease': [
            {'text': 'æ€¥æ€§å¿ƒè‚Œæ¢—æ­»', 'confidence': 0.95, 'start': 0, 'end': 6},
            {'text': 'å¿ƒå¾‹å¤±å¸¸', 'confidence': 0.88, 'start': 7, 'end': 11}
        ],
        'anatomy': [
            {'text': 'å¿ƒè‚Œ', 'confidence': 0.85, 'start': 2, 'end': 4}
        ]
    }
    
    candidate_records = [
        {
            'code': 'I21.9',
            'preferred_zh': 'æ€¥æ€§å¿ƒè‚Œæ¢—æ­»ï¼Œæœªç‰¹æŒ‡',
            'level': 3,
            'parent_code': 'I21',
            'category_path': 'I > I21 > I21.9',
            'semantic_text': 'æ€¥æ€§å¿ƒè‚Œæ¢—æ­» | å¾ªç¯ç³»ç»Ÿç–¾ç—… | ICD-10: I21.9',
            'score': 0.85
        },
        {
            'code': 'I47.9',
            'preferred_zh': 'é˜µå‘æ€§å¿ƒåŠ¨è¿‡é€Ÿï¼Œæœªç‰¹æŒ‡',
            'level': 3,
            'parent_code': 'I47',
            'category_path': 'I > I47 > I47.9',
            'semantic_text': 'é˜µå‘æ€§å¿ƒåŠ¨è¿‡é€Ÿ | å¿ƒå¾‹å¤±å¸¸ | ICD-10: I47.9',
            'score': 0.72
        },
        {
            'code': 'I25.9',
            'preferred_zh': 'æ…¢æ€§ç¼ºè¡€æ€§å¿ƒè„ç—…ï¼Œæœªç‰¹æŒ‡',
            'level': 3,
            'parent_code': 'I25',
            'category_path': 'I > I25 > I25.9',
            'semantic_text': 'æ…¢æ€§ç¼ºè¡€æ€§å¿ƒè„ç—… | å¾ªç¯ç³»ç»Ÿç–¾ç—… | ICD-10: I25.9',
            'score': 0.68
        }
    ]
    
    # æ‰¹é‡è®¡ç®—å¢å¼ºç›¸ä¼¼åº¦
    enhanced_results = similarity_service.batch_calculate_similarities(
        query_text, query_entities, candidate_records
    )
    
    print(f"\næŸ¥è¯¢æ–‡æœ¬: {query_text}")
    print(f"æŸ¥è¯¢å®ä½“: {len(query_entities.get('disease', []))} ä¸ªç–¾ç—…, {len(query_entities.get('anatomy', []))} ä¸ªè§£å‰–éƒ¨ä½")
    
    print(f"\nå¢å¼ºç›¸ä¼¼åº¦ç»“æœ (Top {len(enhanced_results)}):")
    for i, (record, enhanced_score, factors) in enumerate(enhanced_results, 1):
        original_score = record.get('original_score', 0.0)
        improvement = enhanced_score - original_score
        
        print(f"\n{i}. {record['code']}: {record['preferred_zh']}")
        print(f"   åŸå§‹åˆ†æ•°: {original_score:.4f}")
        print(f"   å¢å¼ºåˆ†æ•°: {enhanced_score:.4f} ({improvement:+.4f})")
        print(f"   å±‚çº§: Level {record['level']}")
        print(f"   å› å­è´¡çŒ®:")
        print(f"     å‘é‡ç›¸ä¼¼åº¦: {factors.vector_similarity:.3f}")
        print(f"     å±‚çº§å¢å¼º: {factors.hierarchy_boost:.3f}")
        print(f"     å®ä½“åŒ¹é…: {factors.entity_match_score:.3f}")
        print(f"     è¯­ä¹‰ä¸€è‡´æ€§: {factors.semantic_coherence:.3f}")
    
    # è·å–è¯¦ç»†è§£é‡Š
    if enhanced_results:
        top_result = enhanced_results[0]
        explanation = similarity_service.get_similarity_explanation(top_result[2])
        
        print(f"\nğŸ“Š æœ€ä½³åŒ¹é…çš„è¯¦ç»†åˆ†æ:")
        print(f"æ€»åˆ†: {explanation['total_score']:.4f}")
        for factor_name, factor_info in explanation['factors'].items():
            print(f"  {factor_info['description']}: {factor_info['score']:.3f} Ã— {factor_info['weight']:.2f} = {factor_info['contribution']:.4f}")


if __name__ == "__main__":
    main()